input {
	redis {
  		host => "192.168.99.100"
  		data_type => "list"
  		key => "filebeat"
		codec => json {
	    	charset => "CP1252"
    	}
	}
}

filter {
	if [type] == "searchlogger" {

		# IMPORTANT:
		# When using the redis-input plugin (and filebeat), the actual logdata (in our case, a json-object) 
		# is passed to logstash, wrapped in an json-object: 
		json {
    		source => "message"
    		remove_field  => ["message"]    		
		}

		if ( [logdata][message][Session][ClientIp] =~ /^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$/ ) {

			geoip {
				database => "/data/geoip/GeoLiteCity.dat" 
				source => "[logdata][message][Session][ClientIp]"				
				#target => "geoip_client"
			}
		}
				
		mutate { 
			convert => { "[logdata][message][Result][Hits]" => "integer" }	
		}

		kv { 
			field_split  => "&?"
			source 		 => "[logdata][message][RequestUri]" 
			include_keys => [ "searchDirection", "pageNumber" ]
			target 		 => "[logdata][message][Query][Params]"
		}
		
		date {            
            match => ["[logtime]", "YYYY-MM-dd HH:mm:ss.SSSS"]
            target => "@timestamp"
			#add_field => { "debug" => "timestampMatched"}
        }
	}
}

output {
	#stdout { codec => dots }
	#stdout { codec => rubydebug }	

	if [type] == "searchlogger"  {
		#if "_grokparsefailure" not in [tags] {
			elasticsearch {		
				hosts => ["http://elasticsearch:9200"]
				index => "logstash-%{+YYYY.MM.dd}"
			}
		#}
	}	
}


